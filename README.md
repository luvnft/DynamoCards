# TASKS OVERVIEW :

## Task 1 : Google Cloud, SDK Authentication and Vertex AI
For the first task, we create our Google Cloud Platform account and a project associated with the following account and a service account for managing the permissions associated with the project on the Google cloud platform.

## Task 2 : Dev Environment Setup
For the following task, do check if you are able to access the following GCP account with your service account key from the code provided. Also create an environment inside which you can install all the requirements.Use the service account key to authenticate and authorize access to your GCP resources.Test the access by running a script or command that interacts with GCP services using the credentials provided in the service account key.Hence, Once you have set up the environment and installed the dependencies, run your application to ensure it functions correctly.

## Task 3 : Fast API Initialization 
We start with creating an instance of FastAPI, then we initialize the Gemini Processor to use the gemini-pro model. After this POST ‘/analyze_video’ endpoint is created that accepts a youtube link and generates a unique video id and url_id and converts the youtube link to a string. Youtube processor is initialized with Gemini processor and we try to retrieve the youtube documents as result from the following video. Also ‘/root’ endpoint is added for simple health checks.

## Task 4 : Allowing Cross Origin Request using CORS middleware:
We use CORSMiddleware to allow cross-origin requests which also  allows origins, credentials, methods, and headers. Then we install Axios and create a POST route to handle the video analysis request. We also handle the link change event and send the link to the backend endpoint: analyze_video. After that we  handle the response from the backend and build the input field JSX to display the response data. At last, we test the integration by enabling both the frontend and backend servers and sending a request from the frontend.

## Task 5 : Organizing Tools and Functions
We create another folder named ‘services’ in the backend directory and create an init.py file and a genai.py file where all the tools and functions related to generative AI will be organized. In the genai.py file, Youtube Processor is created with an init method where we use Recursive Character Text Splitter as text splitter. Parameters for the recursive character text splitter include chunk size to be 1000 and chunk overlap to be zero. We also use retrieve_youtube_documents with parameters as video_url and verbose that is used for splitting the documents received from the youtube url and accessing the author, length, title and total billable characters of the following video. Here the total billable characters can be extracted based on the total count of the tokens of the documents received from the video_url. For video information and getting the author , length and result we use verbose as true. We then use this Youtube Processor in the main.py file so that while analyzing the video, we can use the Youtube Processor for retrieving youtube documents.

## Task 6 : Generative AI Document summaries from Youtube url.
Within the genai.py file, we use load_summarize_chain and Vertex AI classes from langchain. Gemini Processor is created with an init method to set self.model as the Vertex AI class where the model name and the project name are set as parameters. Generate_document_summary method is created which takes in parameters as documents and verbose. Within the method, a chain is created using the load_summarize_chain function where we use the Vertex AI model and where the chain type is set to map_reduce if the number of documents is greater than 10, otherwise the chain type is set to “stuff”. In the main.py file we use the Gemini Processor and create the new instance of GeminiProcessor and return the summary of the retrieved documents using the generate_document_summary method. We can check for the summary of the retrieved documents by enabling both frontend and backend servers and sending a url request from the frontend.
 
## Task 7 : Enhancing Youtube Processing Class - Semantic Extraction Algorithm
Youtube Processor is modified to include the method for finding key concepts with parameters as documents and group size. Within the method, we check if the group size is greater than the number of documents, if it is true then we raise a value error,  else we calculate the number of documents per group using the group_size parameter. Then we split the documents into chunks of size: number of  documents per group. For each group, the content of each document is combined in the group into a single string and a Prompt template is created with the template and input variables parameters. Prompt template contains the instructions for the prompt that helps in  finding key concepts or terms and definitions. We add the output of the chain to the list of the batched_concepts. Youtube Processor is also modified to include the genai_processor parameter of type Gemini Processor and in the main.py file genai_processor is passed to the Youtube Processor.  Summary method from before is commented and the endpoint is modified to return the key concepts after invoking find_key_concepts method.

## Task 8 : Billing Character Calculation Enhancement - Semantic Extraction Algorithm
Count_total_tokens method in the Gemini Processor is modified to return the total number of billable characters. Youtube Process is also modified to include the verbose parameter and for logging the total billable characters in the retrieve_youtube_documents method. Now if the verbose is set true, we log the input and the output characters for each group multiplied by the billable characters cost, producing the cost of our semantic extraction algorithm, as well as the batch cost.

## Task 9 : Key Concepts Refactoring and Output Formatting
In the following task the group_size is renamed to sample size in the find_key_concepts method. Now if the number of documents per group is greater than 10 we raise a value error and if the number of documents per group is greater than 5, it provides a warning to the user that the output quality may be degraded. This is based on the fact that the fewer the number of documents per group, the better the output quality. Also the default sample size is zeros where we set the number of documents per group to be 5. Also the prompt template is introduced so that the Gemini model responds as a JSON object containing key concepts and their definition without any backticks separating each concept with a comma. Before returning the key concepts, each JSON string in batch_concepts is converted to a python dictionary as the output of the function. We also add a try and catch clause for the results that doesn’t contain key concepts and definitions so that it doesn’t stop the process at an individual failure.

## Task 10 : Front-End Integration and Flashcard Handling
In the front end, App.jsx , sendLink method is modified to set the key_concepts state to the response data. Other than this, we create a discardFlashcard function for removing a flashcard from the key_concepts state. Flash card component is modified to include an onDiscard parameter and pass the discardFunction to the on Discard parameter.

## Task 11: Adding Memory Cache 
In the main.py we now first check if the following video_url has been previously analyzed or not by checking the video_analysis.db if the following database doesn’t have the video_url then we have to find the key concepts and definition and if the following video url is available in the database then we retrieve the key concepts and definition from the database.

##Task 12 : Docker for the frontend and backend
Under the frontend directory, a Dockerfile that helps in copying applications files to the container, installing Node.js dependencies and builds the application, exposes the port 5173 for external excess and starts the application using the development server.
Under the frontend directory, a Dockerfile that helps in copying applications files to the container, installing Python dependencies from a requirements.txt file, setting up an environment variable for Google application credentials , exposes the port 8000 for external excess and starts the application using Uvicorn with auto-reload enabled for development.
Other than this, we use docker compose which allows both services to run in separate containers and communicate with specified port mappings.
